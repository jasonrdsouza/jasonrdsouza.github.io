{"date":"2015-02-03","template":"post.mustache","title":"Sorting","tags":["programming","golang"],"description":"Sorting is a fundamental, yet often overlooked aspect of computer science due to its status as an effectively 'solved problem'. The state of the art sorting algorithms at the fingertips of most developers are the result of much research and experimentation. This post serves as a crash course on sorting basics, leading up to an understanding of the modern algorithms.","CORK_url":"post/sorting.html","CORK_reading_time":11,"CORK_lix_readability_score":38,"CORK_ari_readability_score":10,"content":"<p>Most programming languages have an\n<a href=\"http://en.wikipedia.org/wiki/Timsort\">extremely efficient, optimized in nearly all cases</a>,\nbuilt in sorting algorithm that's a simple function call away.\nPractically, this removes the need to spend much time contemplating the\nimplementation, efficiencies, and tradeoffs\nbehind the many existing ways to sort,\nand instead focus on the \"actual problem\".\nNonetheless, it is informative to walk through the basics of sorting\nin order to gain a more comprehensive understanding of the current state of the art algorithms.</p>\n<p>Before diving into specifics, there are 3 things to note about sorting in general.</p>\n<ol>\n<li>All general purpose sorting algorithms fall into the category of <a href=\"http://en.wikipedia.org/wiki/Comparison_sort\">comparison sorts</a>, meaning that they depend solely on the ability to compare any two elements and determine which one should come first in the sorted result, and the ability to rearrange elements.</li>\n<li>Sorting algorithms can either use additional buffer space to do their work, or perform the sort \"in place\". In place sorts use constant (or occasionally logarithmic) additional storage space, as opposed to <code>O(n)</code> or more additional memory via a working buffer for regular implementations.</li>\n<li>All <strong>comparison</strong> based sorting algorithms require at least <a href=\"http://en.wikipedia.org/wiki/Time_complexity#Linearithmic_time\">linearithmic</a> asymptotic running time.</li>\n</ol>\n<p>The comparison sorting properties are codified in an <a href=\"http://golang.org/pkg/sort/#Interface\">interface</a> like the following:</p>\n<pre><code class=\"language-go\">type ComparisonSortInterface interface {\n    // Less reports whether the element with\n    // index i should sort before the element with index j.\n    Less(i, j int) bool\n    // Swap swaps the elements with indexes i and j.\n    Swap(i, j int)\n}\n</code></pre>\n<p>All the comparison based sorting implementations below will depend solely on that interface.</p>\n<h3 id=\"selection-sort\">Selection Sort</h3>\n<p>Upon learning of the \"sorting problem\",\nthis is the algorithm that many students initially devise.\nEssentially, the idea is to <em>select</em> the smallest element,\nand move it to the front of the input collection.\nThen, select the next smallest element,\nand move it to the second place in the collection,\ncontinuing in this manner until the entire collection is sorted.\nIn code, it looks like the following:</p>\n<pre><code class=\"language-go\">func SelectionSort(data sort.Interface, a, b int) {\n    if b &lt;= a {\n        return\n    }\n    smallestElementIdx := a\n    for i := a + 1; i &lt; b; i++ {\n        if data.Less(i, smallestElementIdx) {\n            smallestElementIdx = i\n        }\n    }\n    data.Swap(a, smallestElementIdx)\n\n    SelectionSort(data, a+1, b)\n}\n</code></pre>\n<p>Unfortunately, this algorithm is not very efficient,\nwith a best and worst case time complexity of <code>O(n^2)</code>.\nAs written, the algorithm performs the sort in place,\nbut since it is implemented recursively,\nand Go does not optimize <a href=\"http://en.wikipedia.org/wiki/Tail_call\">tail recursion</a>,\nit has a linear memory requirement via the call stack.\nGo is efficient enough that the recursive implementation does not matter in practice\n(the sort becomes impractically slow due to the quadratic runtime before it approaches the stack space limit),\nand I like the elegance of the recursion more than an extra loop, so I left it as is.</p>\n<h3 id=\"bubble-sort\">Bubble Sort</h3>\n<p><img src=\"/static/posts/bubblesort.gif\" alt=\"Bubble Sort Animation\" /></p>\n<p>This is another basic sorting algorithm.\nThe idea is to <em>bubble</em> larger elements towards the end of the list,\nperforming this bubbling until the entire collection is sorted.\nIn code:</p>\n<pre><code class=\"language-go\">func BubbleSort(data sort.Interface, a, b int) {\n    performedSwap := false\n    for i := a + 1; i &lt; b; i++ {\n        if data.Less(i, i-1) {\n            performedSwap = true\n            data.Swap(i, i-1)\n        }\n    }\n\n    if performedSwap {\n        // we know that the largest element is at the end, now sort the rest of the list\n        BubbleSort(data, a, b-1)\n    }\n    // if no swap was performed, the list is sorted\n    return\n}\n</code></pre>\n<p>This algorithm has similar performance characteristics to the Selection Sort above,\nbut with the nicety that if the collection is already sorted,\nit finishes in <code>O(n)</code> time.</p>\n<h3 id=\"insertion-sort\">Insertion Sort</h3>\n<p><img src=\"/static/posts/insertion-sort.gif\" alt=\"Insertion Sort Animation\" /></p>\n<p>Insertion sort works by going through the collection,\nand <em>inserting</em> each element into a sorted version of the collection,\nuntil the entire thing is sorted.\nInsertion sort is usually performed in place,\nmoving elements around in the collection to keep the sorted version at the front.</p>\n<pre><code class=\"language-go\">func InsertionSort(data sort.Interface, a, b int) {\n    for i := a + 1; i &lt; b; i++ {\n        for j := i; j &gt; a &amp;&amp; data.Less(j, j-1); j-- {\n            data.Swap(j, j-1)\n        }\n    }\n}\n</code></pre>\n<p>Efficiency is on par with the first 2 algorithms,\nwith a best and worst case of <code>O(n^2)</code>.\nNote that this particular implementation is in-place,\nand eschews recursion, therefore using no additional memory.</p>\n<h3 id=\"merge-sort\">Merge Sort</h3>\n<p><img src=\"/static/posts/merge-sort.gif\" alt=\"Merge Sort Animation\" /></p>\n<p>While the first 3 sorts were fairly simple and straightforward,\nthis one is more complex.\nAccompanying its complexity, however, is better runtime efficiency.\nMerge sort works by recursively <em>merging</em> sorted subcollections of the input collection.\nEffectively, this means that it takes the input collection,\nand breaks it down into sub-collections that are sorted\n(the base case being a collection with just 1 element, which is sorted by default).\nSubsequently, these sub-collections are merged back together,\nmaintaining the sorted order,\nresulting in a larger, sorted sub-collection.\nEventually, this process results in the entire collection being sorted.</p>\n<pre><code class=\"language-go\">func MergeSort(data sort.Interface, a, b int) {\n    if b-a &lt; 2 {\n        return\n    }\n\n    mid := a + (b-a)/2\n    MergeSort(data, a, mid)\n    MergeSort(data, mid, b)\n    merge(data, a, mid, b)\n}\n\n// Merges (in place) the two sorted logical collections represented by the indices\n// [a, m) and [m, b) of the input data collection. The result is a sorted logical\n// collection that extends from a to b.\nfunc merge(data sort.Interface, a, m, b int) {\n    // check if the two collections are already sorted\n    if data.Less(m-1, m) {\n        return\n    }\n\n    i, j := a, m\n    for i &lt; j &amp;&amp; j &lt; b {\n        if data.Less(i, j) {\n            i++\n        } else {\n            j++\n            shiftRight(data, i, j)\n            // update i pointer to take the shift into account\n            i++\n        }\n    }\n    // whatever remains is already in the right place\n}\n\n// Shifts the data elements in the collection slice data[a,b) to the right\n// by one element, with wraparound, meaning the last element in the slice is now\n// the first element\nfunc shiftRight(data sort.Interface, a, b int) {\n    for i := b - 1; i &gt; a; i-- {\n        data.Swap(i, i-1)\n    }\n}\n</code></pre>\n<p>Merge sort typically has <code>O(nlogn)</code> time complexity in the best and worst case,\nmaking it asymptotically optimal.\nA regular implementation uses <code>O(n)</code> additional space to perform the sort.\nAn in-place implementation, like the one above, reduces that to <code>O(logn)</code> additional space,\nwith the tradeoff of increased running time due to additional swaps\n(since it doesn't use an extra buffer, the reordering of elements is not as simple or efficient).</p>\n<p>The in-place merge function implemented above is simple, but fairly naive.\nAs a result, it has the potential to degrade the runtime performance significantly\n(to <code>O(n^2)</code> in the worst case)\ndepending on the distribution of the input.\nMore sophisticated in place merge functions such as <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.4612\">Symmerge</a> can minimize that degradation,\nbut the space vs. time tradeoff is intrinsic.</p>\n<h3 id=\"quicksort\">Quicksort</h3>\n<p><img src=\"/static/posts/quicksort.gif\" alt=\"Quicksort Animation\" /></p>\n<p>Quicksort is one of the most widespread sorting algorithms,\nand was the default sorting approach in Unix, C, and Java for some time.\nIt has been replaced by fancier sorts with better real-world runtimes,\nbut remains very effective in practice.\nThe idea behind its implementation is to choose a \"pivot\" element from the collection at random,\nand push everything less than the pivot to its left,\nand everything greater than the pivot to its right.\nThen, simply recurse on the left and right sub-collections\n(the pivot is in its final sorted place).</p>\n<pre><code class=\"language-go\">func QuickSort(data sort.Interface, a, b int) {\n    if b-a &lt; 2 {\n        return\n    }\n\n    pivot := b - 1\n    finalPivotSpot := a\n    for i := a; i &lt; b; i++ {\n        if data.Less(i, pivot) {\n            data.Swap(i, finalPivotSpot)\n            finalPivotSpot++\n        }\n    }\n    data.Swap(finalPivotSpot, pivot)\n    QuickSort(data, a, finalPivotSpot)\n    QuickSort(data, finalPivotSpot+1, b)\n}\n</code></pre>\n<p>Quicksort's performance characteristics are interesting in the sense that\nit has a worst case asymptotic runtime of <code>O(n^2)</code>,\nbut in practice (on average),\nit operates in <code>O(nlogn)</code> time.\nIt's worst case runtime comes from the fact that\nit is possible to construct an input such that the smallest (or largest) collection element\nis used as a pivot on each recurrence of the algorithm.\nIn such a case, quicksort effectively devolves into a selection sort.\nBarring that very rare situation,\nquicksort performs better than other <code>O(nlogn)</code> algorithms\nbecause of implementation details leading to\nbetter cache locality, and other, non-asymptotically relevant performance benefits.</p>\n<h3 id=\"the-fancier-sorts\">The Fancier Sorts</h3>\n<p>The fancier sorts I have alluded to are\n<a href=\"http://en.wikipedia.org/wiki/Hybrid_algorithm\">hybrid algorithms</a>\nlike <a href=\"http://en.wikipedia.org/wiki/Timsort\">Timsort</a>\nand <a href=\"http://en.wikipedia.org/wiki/Introsort\">Introsort</a>.\nThe general idea of this class of sorting algorithm is that asymptotic complexity, while important,\nis not the only factor determining the speed of a sorting algorithm in real world workflows.\nDifferent algorithms are optimal in different scenarios,\nand we can take advantage of that knowledge by choosing the most efficient operations\ngiven relevant characteristics of the input data.\nFor example, simple sorts like insertion sort perform better than more complex sorts when dealing with small datasets.\nAs the data size grows, quicksort may become the best option.\nOnce a certain call stack level is reached,\nit may make sense to switch again to something like merge sort, and so on.</p>\n<p>A perspicuous example of hybrid sorting algorithms in practice\nis the <a href=\"http://golang.org/src/sort/sort.go\">default sort in Go</a>.\nFor a fascinating historical perspective,\n<a href=\"http://svn.python.org/projects/python/trunk/Objects/listsort.txt\">here's the inventor of Timsort, Tim Peters' original paper describing his sort implementation</a>,\nwhich went on to be the standard sorting algorithm in Python, Java, and many other standard libraries.</p>\n<h3 id=\"non-comparison-based-sorts\">Non-Comparison Based Sorts</h3>\n<p>I said earlier that all general purpose sorting algorithms are comparison based.\nIf, however, restrictions are placed on the input,\nit is possible to sort without comparisons.\nAdditionally, non-comparative sorts have the potential to achieve better optimal run time efficiencies.</p>\n<p>A common type of non-comparative sort is <a href=\"http://en.wikipedia.org/wiki/Radix_sort\">Radix Sort</a>.\nLoosely, this sorting algorithm works by making a single pass through the input collection,\nand putting its elements into pre-ordered buckets.\nThe reason this technique doesn't work on arbitrary input is that the buckets must be created before running the sort,\nand so the input data must be restricted in some way such that it is known <em>a priori</em> how many buckets to create.</p>\n<p>In spite of this restriction, these sorts are useful in many situations.\nFor example, imagine wanting to sort a large collection of people objects based on their age (in years).\nTypical intuition would say that the best option would have <code>O(nlogn)</code> asymptotic time complexity.\nIf, however, it's reasonable to assume that ages are restricted from 0 to 250 years,\nthe sort can be accomplished in what is effectively <code>O(n)</code> time using a bucket sorting methodology.\nSpecifically, create an <code>ageBuckets</code> array with a length of 250 -- a bucket for each year.\nThen, iterate through the input collection,\nputting each person into the bucket corresponding to their age.</p>\n<p>This method takes <code>O(n)</code> time for the sort,\nand another <code>O(n)</code> to create a sorted collection from the buckets.\nTechnically, because we are examining the <em>value</em> of each element,\nthe real running time is <code>O(kn)</code>, where <code>k</code> is the size (length of the representation) of the values,\nand for arbitrarily large values, <code>k</code> can grow to be of <code>log(n)</code> size,\nso it can be argued that the real asymptotic complexity of a bucketing sort is still <code>O(nlogn)</code>,\nbut we'll leave that particular exploration for another post.</p>\n<p>For most cases where it is practical,\na bucketing sort is a nice speed win,\nwith the tradeoff of additional memory usage.</p>\n<p><em>The source code (along with tests) for the sorting algorithms presented here can be found on <a href=\"https://gist.github.com/jasonrdsouza/aa84c11eee0d8cb1a981\">Github</a>.</em></p>\n"}